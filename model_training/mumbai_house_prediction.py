# -*- coding: utf-8 -*-
"""Mumbai_House_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12oQRdz9TBKik5hE_VoXa3iIHEeP2xAmO
"""

import pandas as pd
import numpy as np

df = pd.read_csv("./Mumbai_House_Prices.csv")

df.head(3)

# converting prices to INR
def convert_price_to_inr(price, unit):
  if unit == "L":
    return price * 1e5 # 1 Lakh = 100,000
  elif unit == "Cr":
    return price * 1e7 # 1 Crore = 10,000,000
  else:
    return np.nan # Not a number

df["price_in_inr"] = df.apply(lambda x: convert_price_to_inr(x["price"], x["price_unit"]), axis = 1)

# Drop columns
df.drop(columns = ["price", "price_unit"], inplace = True)

# Rename price_in_inr to price
df.rename(columns={"price_in_inr" : "price"}, inplace=True)

df.head(3)

df.shape

df.info()

# visualization of dataset (optional)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,6))
sns.boxplot(x=df["area"])
plt.title("Box plot of Area")
plt.show()

plt.figure(figsize=(10,6))
sns.boxplot(x=df["price"])
plt.title("Box plot of Price")
plt.show()

# Function to remove outliers using IQR method
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

df_cleaned = remove_outliers(df, "area")

df_cleaned = remove_outliers(df_cleaned, "price")

df_cleaned.shape

df_cleaned.head(3)

from scipy import stats
df_cleaned['area_zscore'] = np.abs(stats.zscore(df_cleaned['area']))
df_cleaned['price_zscore'] = np.abs(stats.zscore(df_cleaned['price']))
threshold = 3
df2 = df_cleaned[(df_cleaned['area_zscore'] < threshold) & (df_cleaned['price_zscore'] < threshold)].copy()
df2.drop(columns=['area_zscore', 'price_zscore'], inplace=True)
df2.reset_index(drop=True, inplace=True)

df2.shape

df2.head(3)

categorical_vars = ['locality', 'region', 'status', 'age']
for var in categorical_vars:
  print(f"Unique values and counts for {var}: ")
  print(df2[var].value_counts())

# Group less frequent valuese into Other
def group_low_count_categories(series, threshold):
  counts = series.value_counts()
  mask = series.isin(counts[counts >= threshold].index)
  series = series.mask(~mask, "Other")
  return series

df2["locality"] = group_low_count_categories(df2["locality"], threshold = 50)
df2["region"] = group_low_count_categories(df2["region"], threshold = 100)

print("Updated unique values and counts for locality after grouping:")
print(df2['locality'].value_counts(),"\n")

print("Updated unique values and counts for region after grouping:")
print(df2['region'].value_counts())

# Optional
import json
# Function to save DataFrame columns as JSON
def save_columns_as_json(df, columns, filename):
    for col in columns:
        unique_values = df[col].unique().tolist()
        with open(f'{filename}_{col}.json', 'w') as f:
            json.dump(unique_values, f)

# Example usage: Save 'type', 'status', 'age', 'locality', 'region' columns as JSON
save_columns_as_json(df, ['type', 'status', 'age', 'locality', 'region'], 'unique_values')

print("JSON files saved successfully.")

# Compute mean price for each locality and region
locality_means = df2.groupby("locality")["price"].mean()
region_means = df2.groupby("region")["price"].mean()

# Map mean prices back to the dataframe
df2["locality_target_encoded"] = df2["locality"].map(locality_means)
df2["region_target_encoded"] = df2["region"].map(region_means)

df2[['locality_target_encoded', 'region_target_encoded']].head(3)

import joblib
import pickle

# Extract unique values and their encoded values into dictionaries
locality_encoding_map = dict(zip(df2["locality"], df2["locality_target_encoded"]))
region_encoding_map = dict(zip(df2["region"], df2["region_target_encoded"]))

# Save locality encoding map to file
with open('locality_encoding_map.pkl', 'wb') as f:
  pickle.dump(locality_encoding_map, f)

# Save region encoding map to file
with open('region_encoding_map.pkl', 'wb') as f:
  pickle.dump(region_encoding_map, f)

# Drop original locality and region columns if desired
df2.drop(['locality', 'region'], axis=1, inplace=True)

# Perform one-hot encoding for status and age
df2 = pd.get_dummies(df2, columns=["type", "status", "age"], drop_first=True, dtype=int)
df2.head(3)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

df2_scaled = df2.copy()
df2_scaled[["bhk", "area"]] = scaler.fit_transform(df2_scaled[["bhk", "area"]])

joblib.dump(scaler, "min_max_scaler.pkl")

from sklearn.model_selection import train_test_split

X = df2_scaled.drop("price", axis = 1)
y = df2_scaled["price"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# models

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.datasets import make_regression

models = {
      'Linear Regression': (LinearRegression(), {
      'fit_intercept': [True, False],
      'copy_X': [True, False]
    }),
      'DecisionTree Regressor': (DecisionTreeRegressor(), {
      'max_depth': [None, 10, 20, 30, 50],
      'min_samples_split': [2, 5, 10],
      'min_samples_leaf': [1, 2, 5, 10]
    }),
        'Support Vector Regression': (SVR(), {
        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
        'C': [0.1, 1.0,],
        'epsilon': [0.1, 0.5, 1.0],
        'gamma': ['scale', 'auto'],
        'degree': [2, 3, 4]
    })
}

# Track the best model and its score
best_model = None
best_score = -float('inf')
best_model_name = None

# Perform GridSearchCV for each model
for name, (model, param_grid) in models.items():
  grid_search = GridSearchCV(model, param_grid, scoring="r2", cv=3, verbose=1, n_jobs=-1)
  grid_search.fit(X_train, y_train)

  print(f"Best parameters for {name}: {grid_search.best_params_}")
  print(f"Best cross-validation score for {name}: {grid_search.best_score_:.4f}\n")

  # Evaluate on test set using the R^2 score
  test_score = grid_search.best_estimator_.score(X_test, y_test)
  print(f"Test set score (R^2) for {name}: {test_score:.4f}\n")

  # Check if this model is the best so far
  if grid_search.best_score_ > best_score:
      best_score = grid_search.best_score_
      best_model = grid_search.best_estimator_
      best_model_name = name

# Save the best model using joblib
if best_model is not None:
    filename = f"{best_model_name.lower().replace(' ', '_')}_regression_model.pkl"
    joblib.dump(best_model, filename)
    print(f"Saved {best_model_name} model as {filename}")
else:
    print("No best model found.")